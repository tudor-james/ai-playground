{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a4bcb-5f86-4b4f-9a13-08880f6ad576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch datasets accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c7aeb-f605-43aa-9bdf-4fccc798d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import accelerate\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fff367-5df0-4c0f-9ec0-59013aa3eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434759d1-5195-4471-8709-b4b2cc537dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='/mnt/sagemaker-nvme/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38e04e-1aa1-4e30-9ac0-5a28f0f64b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d5f88-8577-47f8-922a-740690132e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1b72b-f025-4aec-b992-24b36bcdeab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc44133-d625-4336-88ab-ad02644a0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea67554-a91f-4008-bdcc-fcc24e2d6c8d",
   "metadata": {},
   "source": [
    "# Let's test it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1adf3-fdb0-4553-92a8-7208a5921c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_not_chat = tokenizer.encode_plus(\"[INST] Tell me about robotic process automation? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "\n",
    "generated_ids = model.generate(inputs_not_chat, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2d503-1c0b-4da8-a33e-4807ce75ebf6",
   "metadata": {},
   "source": [
    "# Create the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb884e-6087-4ddf-b7c1-be7b26fb27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain chromadb openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb0a6a-bb5b-466e-845e-32f9508950d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import nest_asyncio\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb335d0-445e-4a47-8082-9ea021fd1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the library\n",
    "loader = CSVLoader(\"../library.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Load chunked documents into the Chroma index\n",
    "db = Chroma.from_documents(chunked_documents, embedding_function)\n",
    "\n",
    "# Connect query to Chroma index using a retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6733c-07b0-4868-9a43-77193e0fb9c2",
   "metadata": {},
   "source": [
    "# Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ed610-a64b-470a-a391-259bc771dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What experience does Methods have in RPA?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617a88c-1c08-4fc0-a180-3edc72d77924",
   "metadata": {},
   "source": [
    "# Create the LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd719b4a-e57c-4631-a523-eb91fc8536b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "knowledge from the files in the vector database only. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8fb3a5-f435-4a19-956e-e4a6d2cb2dc0",
   "metadata": {},
   "source": [
    "# Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7771491-5768-4f6e-b17a-1c8e8178a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the text in the vector database.\" \n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a258c4-9c09-4fd1-b2ec-35ccdbb8baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are Methods known for?\"\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc3c65-0ffc-4d64-abb9-1d62af70b770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
