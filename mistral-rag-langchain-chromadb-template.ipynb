{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81544e7-f1c5-44cf-9faf-d7f2bc852117",
   "metadata": {},
   "source": [
    "# The hitchhiker's guide to Jupyter (part 5/n)\n",
    "\n",
    "## Let's create a Generative AI chatbot using RAG to talk to my book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea7779-0905-41f2-bb52-506fe08be871",
   "metadata": {},
   "source": [
    "I'm going to implement local chatbot on my laptop to talk to my book, [\"Designed4Devops\"](https://designed4devops.com). This will allow a user to be able to ask questions of the book and summarise its contents. My book is self-published and copywrite so it shouldn't appear in models' training data. To achieve this I'm going to RAG or _Retrieval Augmented Generation_. \n",
    "\n",
    "## RAG\n",
    "\n",
    "RAG is a technique that allows you to add data to a LLM after the model was trained, without retraining or finetuning it. Training models requires access to large and often numerous high-end GPUs. This can be expensive. It also has the downside that if you want to update the data, you need to retrain the model again.\n",
    "\n",
    "RAG overcomes this by taking the data (e.g., PDF, CSV, HTML) and vectorising it. Remember that models work by matrix multiplations of numbers not text. We use a model to embed the text as numbers in a vectore store. This allows the LLM to query the data with symantec searching. The model then returns results based on the context of the query given.\n",
    "\n",
    "Let's set up the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238abf1f-e8ff-4913-9d4a-cf53f5a32b15",
   "metadata": {},
   "source": [
    "### First, we'll install the dependencies and set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275d0f8-7cb0-45c8-9fa7-3af48132bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall -Uq torch datasets accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8c7aeb-f605-43aa-9bdf-4fccc798d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import accelerate\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fff367-5df0-4c0f-9ec0-59013aa3eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4826cd-5aca-43e1-bed3-0666ff4dea6a",
   "metadata": {},
   "source": [
    "This sets up the tokeniser. This breaks the text up into tokens (chunks) which can be individual words or fragments of words.\n",
    "\n",
    "I'm going to use Mistral 7B as it offers a good performance at a low overhead of processing and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8569491-9135-4fdb-a930-5171e5647b45",
   "metadata": {},
   "source": [
    "### let's load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a866d7-77f4-416d-98ee-291df4d2546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='../models/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bff158-c603-4bd4-ba4b-5b2d61a8325b",
   "metadata": {},
   "source": [
    "#### Quantization of the Model\n",
    "\n",
    "I'm going to quantize the model to 4 bits. This lowers the precision of the data types (int4 vs fp16 or fp32), which reduces the overheads even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac06130-f66b-4341-9ee9-6197cd2effa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72d5f88-8577-47f8-922a-740690132e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa1b72b-f025-4aec-b992-24b36bcdeab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc44133-d625-4336-88ab-ad02644a0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099710a5301f4df0815ca3e3252b82f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea67554-a91f-4008-bdcc-fcc24e2d6c8d",
   "metadata": {},
   "source": [
    "#### Let's test it..\n",
    "\n",
    "This query asks the model a question. We haven't loaded any of our data into it yet, this is all information held within the model from its training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ade844-6249-4803-8972-6896699c3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\":\"user\",\n",
    "    \"content\": \"Can you tell us 3 reasons why Eryri is a good place to visit?\"\n",
    "}]\n",
    "\n",
    "tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385ac6a5-2105-4e98-ae8c-1ff05c85ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  2418,   368,  1912,   592, 28705, 28770,\n",
       "          6494,  2079,   413,   643,   373,   349,   264,  1179,  1633,   298,\n",
       "          3251, 28804,   733, 28748, 16289, 28793]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e1adf3-fdb0-4553-92a8-7208a5921c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Can you tell us 3 reasons why Eryri is a good place to visit? [/INST] 1. Scenic Beauty: Eryri is known for its stunning natural landscapes and picturesque views. From the majestic mountains to the serene lakes and rolling hills, the region offers a range of breathtaking sights to behold. The area is also home to several national parks and nature reserves, providing visitors with opportunities to hike, bike, and explore the great outdoors.\n",
      "\n",
      "2. Rich Cultural Heritage: Eryri has a rich and diverse cultural heritage, with a number of ancient sites, historic buildings, and museums to explore. The region is home to several castles, including the famous Conwy Castle, which dates back to the 13th century. Visitors can also learn about the region's Celtic and medieval history at the National Museum of Wales in Conwy.\n",
      "\n",
      "3. Delicious Food and Drink: Eryri is known for its delicious cuisine, which combines traditional Welsh flavors with modern twists. The region is famous for its lamb, beef, and seafood, and visitors can sample these local specialties at a number of restaurants, cafes, and markets. The area is also home to several craft breweries and distilleries, offering visitors the chance to taste some of the region's finest beverages.</s>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens = 1000,\n",
    "    do_sample = True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids,padding=True)\n",
    "\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3907e9-d871-4f3e-b605-142c42d859cf",
   "metadata": {},
   "source": [
    "___The model is working!___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0baba-0611-4acc-b0f6-8d436bb6fac5",
   "metadata": {},
   "source": [
    "#### Create the vector database\n",
    "\n",
    "I'm going to use ChromaDB, which is a lightweight local vector store, to hold the embeddings of the books text that will come from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb884e-6087-4ddf-b7c1-be7b26fb27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall -Uq langchain chromadb openai tiktoken sentence-transformers pypdf fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a052fb6d-8a53-4894-b1d4-e3de5173d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb335d0-445e-4a47-8082-9ea021fd1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6d18880a8e49a4bddb77d96b2e3527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Load the book\n",
    "loader = PyPDFLoader(\"/tf/docker-shared-data/rag-data/d4do_paperback.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "chunks[0]\n",
    "\n",
    "store = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    ids = [f\"{item.metadata['source']}-{index}\" for index, item in enumerate(chunks)],\n",
    "    collection_name=\"D4DO-Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6733c-07b0-4868-9a43-77193e0fb9c2",
   "metadata": {},
   "source": [
    "#### Test the vector store\n",
    "\n",
    "This tests that the data exists within the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53cab25f-db99-4b71-af05-90bd7b670d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context of discussions that happened at the time. For example, an architect, a UI designer, and a developer\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Conway say?\"\n",
    "docs = store.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab066c97-72e4-4127-80bb-e49533bb266c",
   "metadata": {},
   "source": [
    "#### Test the model with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d5229a-0276-4efb-8ca2-0c40d1ba49bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Act as a consultant. I have a client who needing to make his software product company more efficient.     I want to impress my client by providing advice from the book Designed4Devops.     What do you recommend?     Give me two options, along with how to go about it for each [/INST] Option 1: Automation and Continuous Integration\n",
      "\n",
      "One way to improve efficiency in a software product company is to implement automation and continuous integration (CI) practices. This can help streamline the development process, reduce manual errors, and improve the speed and reliability of software releases.\n",
      "\n",
      "To do this, the first step is to identify areas of the development process where automation and continuous integration can be implemented. This may include building automated tests, automating the deployment process, and implementing a CI tool, such as Jenkins or Travis CI, to orchestrate these processes.\n",
      "\n",
      "To ensure a successful implementation, it's important to involve the entire development team in the process to ensure that everyone understands the benefits of automation and continuous integration, and to ensure that the tools and processes are tailored to the specific needs of the company.\n",
      "\n",
      "Option 2: DevOps Practices and Culture\n",
      "\n",
      "Another way to improve efficiency in a software product company is to adopt DevOps practices and culture. This involves breaking down the traditional silos between development, operations, and IT teams, and promoting a culture of collaboration and communication between these teams.\n",
      "\n",
      "To do this, the first step is to assess the current state of practices and culture within the company. This can help identify areas where changes are needed, such as improving communication and collaboration between teams, implementing automation and continuous integration practices, and adopting a version control system, such as Git, to version control code and ensure that changes can be tracked and managed consistently.\n",
      "\n",
      "To ensure a successful implementation, it's important to involve the entire organization in the process and to provide training and support to help employees adopt the new practices and culture.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"Act as a consultant. I have a client who needing to make his software product company more efficient. \\\n",
    "    I want to impress my client by providing advice from the book Designed4Devops. \\\n",
    "    What do you recommend? \\\n",
    "    Give me two options, along with how to go about it for each\"\n",
    "}]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages,return_tensors = \"pt\",padding=True).to('cuda:0')\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens = 1000,\n",
    "    do_sample = True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617a88c-1c08-4fc0-a180-3edc72d77924",
   "metadata": {},
   "source": [
    "#### Create the LLM chain\n",
    "\n",
    "To create a symantically aware search, we need to store the context of the question, and engineer a prompt that focuses the model on answering questions using the data from our vector store instead of making it up (hallucinating). Prompt engineering is a way to coach the model into giving the sort of answers that you want return and filter those that you don't. This block sets up the chain and the template for the query that brings the context and question together to engineer the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd719b4a-e57c-4631-a523-eb91fc8536b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "designed4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52722d11-4370-4216-bbc3-946d679d38ad",
   "metadata": {},
   "source": [
    "#### Create RAG Chain\n",
    "\n",
    "This chain allows us to engineer our prompt by adding context to the question to _hopefully_ get a stronger answer. The context will come from our vector store where we embedded the book as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd6f56c-41d3-4dee-885c-4df41692b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever()\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ffbf2-ffd4-463d-9cb4-6e9e60923afa",
   "metadata": {},
   "source": [
    "## The Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f27d5e3f-9a5b-4f8b-8e54-acf50c98fca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='we process changes to our product. Digital marketplaces move quickly, so we need to introduce change', metadata={'page': 32, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='process and within the product to create short feedback loops that allow us to keep improving our product', metadata={'page': 230, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='concern yet. Improving them comes later. If you can identify separate workflows within your product, you', metadata={'page': 57, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='making minor changes and integrating, testing, and releasing them more often.', metadata={'page': 125, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})],\n",
       " 'question': 'How do I improve the speed of changes that we are making to my product?',\n",
       " 'text': \"\\n### [INST] \\nInstruction: Answer the question based on your \\ndesigned4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\\n\\n[Document(page_content='we process changes to our product. Digital marketplaces move quickly, so we need to introduce change', metadata={'page': 32, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='process and within the product to create short feedback loops that allow us to keep improving our product', metadata={'page': 230, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='concern yet. Improving them comes later. If you can identify separate workflows within your product, you', metadata={'page': 57, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='making minor changes and integrating, testing, and releasing them more often.', metadata={'page': 125, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})]\\n\\n### QUESTION:\\nHow do I improve the speed of changes that we are making to my product? \\n\\n[/INST]\\n \\nThere are several ways to improve the speed of changes being made to a product. One approach is to break down the product into smaller, manageable workflows that can be improved independently. This allows for faster iteration and testing of individual components, which can lead to quicker overall improvements. Additionally, implementing continuous integration and delivery practices can help streamline the development process and reduce the time it takes to release new features or updates.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I improve the speed of changes that we are making to my product?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb77ca0-2e37-446d-af28-02b1f1833558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='delivery before the physical installation. Anything we can do to streamline this workflow will significantly', metadata={'page': 158, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='continuous flow of minor changes. You might charge on a per-transaction for APIs or per-user for', metadata={'page': 100, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='existing pipelines to increase flow. Mapping a value stream from an idea to release or a backlog entry to', metadata={'page': 54, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='difficult to automate with continuous integration and deployment tools, which slows the flow. \\nMicroservices', metadata={'page': 127, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})],\n",
       " 'question': 'How do I speed up the transfer of workflow tickets for new releases, from the development team to the operations team?',\n",
       " 'text': \"\\n### [INST] \\nInstruction: Answer the question based on your \\ndesigned4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\\n\\n[Document(page_content='delivery before the physical installation. Anything we can do to streamline this workflow will significantly', metadata={'page': 158, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='continuous flow of minor changes. You might charge on a per-transaction for APIs or per-user for', metadata={'page': 100, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='existing pipelines to increase flow. Mapping a value stream from an idea to release or a backlog entry to', metadata={'page': 54, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='difficult to automate with continuous integration and deployment tools, which slows the flow. \\\\nMicroservices', metadata={'page': 127, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})]\\n\\n### QUESTION:\\nHow do I speed up the transfer of workflow tickets for new releases, from the development team to the operations team? \\n\\n[/INST]\\n \\nThere are several ways to speed up the transfer of workflow tickets for new releases from the development team to the operations team. One approach is to use a continuous integration and deployment (CI/CD) pipeline that automates the process of building, testing, and deploying code changes. This can help reduce the time and effort required to manually transfer workflow tickets between teams. Additionally, using microservices architecture can help break down complex applications into smaller, more manageable components, making it easier to automate and streamline processes. Another approach is to map the value stream from an idea to release or a backlog entry to delivery, which can help ensure that all necessary steps are taken to deliver a new release on time.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I speed up the transfer of workflow tickets for new releases, from the development team to the operations team?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65546a1c-2df4-4d41-8e75-095a040daf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='negotiation from your pipeline and remove the dependency. It removes waste. You do need to exercise', metadata={'page': 100, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='lower the risk of waste creeping into the pipelines, products, and processes. When we get down to the flow', metadata={'page': 53, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='methodologies to optimize production lines, reduce waste, and increase the agility they deliver physical', metadata={'page': 25, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='our pipeline and retest the build from scratch. It is adding waste to the system and potentially', metadata={'page': 138, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})],\n",
       " 'question': 'How do I reduce the waste in my delivery pipelines?',\n",
       " 'text': \"\\n### [INST] \\nInstruction: Answer the question based on your \\ndesigned4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\\n\\n[Document(page_content='negotiation from your pipeline and remove the dependency. It removes waste. You do need to exercise', metadata={'page': 100, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='lower the risk of waste creeping into the pipelines, products, and processes. When we get down to the flow', metadata={'page': 53, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='methodologies to optimize production lines, reduce waste, and increase the agility they deliver physical', metadata={'page': 25, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='our pipeline and retest the build from scratch. It is adding waste to the system and potentially', metadata={'page': 138, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})]\\n\\n### QUESTION:\\nHow do I reduce the waste in my delivery pipelines? \\n\\n[/INST]\\n \\nThere are several ways to reduce waste in delivery pipelines. One approach is to negotiate with your team to remove dependencies that are not necessary for the project. This can help streamline the pipeline and reduce unnecessary steps. Additionally, it's important to lower the risk of waste creeping into the pipelines, products, and processes by focusing on the flow and methodologies that optimize production lines, reduce waste, and increase agility. Another way to reduce waste is to retest the build from scratch if a change is made to the pipeline. However, this should be done carefully as it may add additional waste to the system.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I reduce the waste in my delivery pipelines?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ddec316-6230-42ad-8d28-203b94cf5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='designed4devops', metadata={'page': 0, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='how designed4devops  helps to make this easier to implement. It is a more technical discussion of', metadata={'page': 17, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='designed4devops  sets out to increase the flow of novemes through this lifecycle while improving', metadata={'page': 50, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='in the Designed4: Analysis section. \\n \\nApplication Development', metadata={'page': 163, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})],\n",
       " 'question': 'How will Designed4Devops help me cook a risotto withour burning the rice?',\n",
       " 'text': \"\\n### [INST] \\nInstruction: Answer the question based on your \\ndesigned4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\\n\\n[Document(page_content='designed4devops', metadata={'page': 0, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='how designed4devops  helps to make this easier to implement. It is a more technical discussion of', metadata={'page': 17, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='designed4devops  sets out to increase the flow of novemes through this lifecycle while improving', metadata={'page': 50, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='in the Designed4: Analysis section. \\\\n \\\\nApplication Development', metadata={'page': 163, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})]\\n\\n### QUESTION:\\nHow will Designed4Devops help me cook a risotto withour burning the rice? \\n\\n[/INST]\\n \\nThere is no direct answer to this question in the provided documents. However, it is important to note that Designed4DevOps is a methodology for software development and delivery that aims to improve the efficiency and reliability of the entire software development lifecycle. While it may not directly address cooking a risotto, it can potentially help in implementing automated processes and tools to streamline software development and delivery, which could indirectly lead to better outcomes.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How will Designed4Devops help me cook a risotto withour burning the rice?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c04c3-a51a-4f99-b8c6-8ed42617bb59",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This demonstrates that the barrier to entry for prototyping generative AI servics is surprisingly low. This was achieved on relatively modest compute resources in in a few hours.\n",
    "\n",
    "Before you jump in, be sure to check out my blog on [Generative AI and RAG Security](https://www.linkedin.com/posts/methods_the-hitchhikers-guide-to-jupyter-activity-7195772472221224961-IBzo?utm_source=share&utm_medium=member_desktop).\n",
    "\n",
    "I'll be taking this project further and blogging along the way. I'll be talking about the environment I used to build this demo, how I productionise the system, package it and host it, and adding a front end so that you can interact with the book yourselves!\n",
    "\n",
    "You can download this blog as a Jupyter notebook file [here](https://github.com/tudor-james/ai-playground/blob/main/mistral-rag-langchain-chromadb.ipynb). As ever, if you need help with AI projects you can get in touch with Methods or contact us via LinkedIn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
