{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81544e7-f1c5-44cf-9faf-d7f2bc852117",
   "metadata": {},
   "source": [
    "# The hitchhiker's guide to Jupyter (part 5/n)\n",
    "\n",
    "## Let's create a Generative AI chatbot using RAG to talk to my book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea7779-0905-41f2-bb52-506fe08be871",
   "metadata": {},
   "source": [
    "I'm going to implement local chatbot on my laptop to talk to my book, [\"Designed4Devops\"](https://designed4devops.com). This will allow a user to be able to ask questions of the book and summarise its contents. My book is self-published and copywrite so it shouldn't appear in models' training data. To achieve this I'm going to RAG or _Retrieval Augmented Generation_. \n",
    "\n",
    "## RAG\n",
    "\n",
    "RAG is a technique that allows you to add data to a LLM after the model was trained, without retraining or finetuning it. Training models requires access to large and often numerous high-end GPUs. This can be expensive. It also has the downside that if you want to update the data, you need to retrain the model again.\n",
    "\n",
    "RAG overcomes this by taking the data (e.g., PDF, CSV, HTML) and vectorising it. Remember that models work by matrix multiplations of numbers not text. We use a model to embed the text as numbers in a vectore store. This allows the LLM to query the data with symantec searching. The model then returns results based on the context of the query given.\n",
    "\n",
    "Let's set up the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238abf1f-e8ff-4913-9d4a-cf53f5a32b15",
   "metadata": {},
   "source": [
    "### First, we'll install the dependencies and set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275d0f8-7cb0-45c8-9fa7-3af48132bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --force-reinstall -Uq torch datasets accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8c7aeb-f605-43aa-9bdf-4fccc798d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import accelerate\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fff367-5df0-4c0f-9ec0-59013aa3eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4826cd-5aca-43e1-bed3-0666ff4dea6a",
   "metadata": {},
   "source": [
    "This sets up the tokeniser. This breaks the text up into tokens (chunks) which can be individual words or fragments of words.\n",
    "\n",
    "I'm going to use Mistral 7B as it offers a good performance at a low overhead of processing and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8569491-9135-4fdb-a930-5171e5647b45",
   "metadata": {},
   "source": [
    "### let's load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a866d7-77f4-416d-98ee-291df4d2546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='../models/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bff158-c603-4bd4-ba4b-5b2d61a8325b",
   "metadata": {},
   "source": [
    "#### Quantization of the Model\n",
    "\n",
    "I'm going to quantize the model to 4 bits. This lowers the precision of the data types (int4 vs fp16 or fp32), which reduces the overheads even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac06130-f66b-4341-9ee9-6197cd2effa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72d5f88-8577-47f8-922a-740690132e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa1b72b-f025-4aec-b992-24b36bcdeab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc44133-d625-4336-88ab-ad02644a0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdd12e3030e46a0916425cd8418f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea67554-a91f-4008-bdcc-fcc24e2d6c8d",
   "metadata": {},
   "source": [
    "#### Let's test it..\n",
    "\n",
    "This query asks the model a question. We haven't loaded any of our data into it yet, this is all information held within the model from its training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ade844-6249-4803-8972-6896699c3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\":\"user\",\n",
    "    \"content\": \"Can you tell us 3 reasons why Eryri is a good place to visit?\"\n",
    "}]\n",
    "\n",
    "tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "385ac6a5-2105-4e98-ae8c-1ff05c85ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  2418,   368,  1912,   592, 28705, 28770,\n",
       "          6494,  2079,   413,   643,   373,   349,   264,  1179,  1633,   298,\n",
       "          3251, 28804,   733, 28748, 16289, 28793]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e1adf3-fdb0-4553-92a8-7208a5921c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 20:57:28.126039: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-21 20:57:28.501029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Can you tell us 3 reasons why Eryri is a good place to visit? [/INST] Sure, I'd be happy to help! Here are three reasons why Eryri is a great place to visit:\n",
      "\n",
      "1. Scenic Beauty: Eryri is located in the heart of the Welsh mountains, offering breathtaking views of the surrounding countryside. The area is home to several national parks, including the Snowdonia National Park, which is known for its stunning landscapes, waterfalls, and hiking trails. Whether you're a nature lover or just looking for a peaceful getaway, Eryri is the perfect destination.\n",
      "2. Cultural Attractions: Eryri is steeped in history and culture, with several interesting attractions to explore. The town is home to the Royal Welsh College of Music & Drama, which hosts regular performances and is a great place to experience Welsh culture and arts. You can also visit the Welsh Language Centre, which offers courses and workshops on Welsh language and culture.\n",
      "3. Outdoor Activities: Eryri is a great destination for outdoor enthusiasts, with a range of activities to choose from. The area is popular for hiking, mountain biking, and rock climbing, with several challenging trails to explore in the surrounding countryside. If you're looking for a more leisurely activity, you can also take a scenic drive or go horseback riding in the beautiful Welsh countryside.</s>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens = 1000,\n",
    "    do_sample = True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids,padding=True)\n",
    "\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3907e9-d871-4f3e-b605-142c42d859cf",
   "metadata": {},
   "source": [
    "___The model is working!___\n",
    "\n",
    "Correction, the Royal Welsh College of music and drama is in Cardif, which is not in Eryri, which is not a town. I recommend visiting Harlech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0baba-0611-4acc-b0f6-8d436bb6fac5",
   "metadata": {},
   "source": [
    "#### Create the vector database\n",
    "\n",
    "I'm going to use ChromaDB, which is a lightweight local vector store, to hold the embeddings of the books text that will come from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64eb884e-6087-4ddf-b7c1-be7b26fb27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "datasets 2.19.1 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install --force-reinstall -Uq langchain chromadb openai tiktoken sentence-transformers pypdf fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a052fb6d-8a53-4894-b1d4-e3de5173d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb335d0-445e-4a47-8082-9ea021fd1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48acce90f59441329831df5321f34594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Load the book\n",
    "loader = PyPDFLoader(\"/tf/docker-shared-data/rag-data/d4do_paperback.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "chunks[0]\n",
    "\n",
    "store = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    ids = [f\"{item.metadata['source']}-{index}\" for index, item in enumerate(chunks)],\n",
    "    collection_name=\"D4DO-Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6733c-07b0-4868-9a43-77193e0fb9c2",
   "metadata": {},
   "source": [
    "#### Test the vector store\n",
    "\n",
    "This tests that the data exists within the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53cab25f-db99-4b71-af05-90bd7b670d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context of discussions that happened at the time. For example, an architect, a UI designer, and a developer\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Conway say?\"\n",
    "docs = store.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab066c97-72e4-4127-80bb-e49533bb266c",
   "metadata": {},
   "source": [
    "#### Test the model with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d5229a-0276-4efb-8ca2-0c40d1ba49bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Act as a consultant. I have a client who needing to make his software product company more efficient.     I want to impress my client by providing advice from the book Designed4Devops.     What do you recommend?     Give me two options, along with how to go about it for each [/INST] To improve the efficiency of the software product company, I would recommend considering two options from the book \"Designed for DevOps\":\n",
      "\n",
      "Option 1: Adopt a DevOps approach\n",
      "\n",
      "A DevOps approach involves a set of practices that promote collaboration and communication between development and operations teams to improve the efficiency and reliability of software products. To adopt a DevOps approach, your client should consider the following steps:\n",
      "\n",
      "1. Identify and eliminate silos between development and operations teams. Encourage collaboration between teams by fostering a culture of communication and shared goals.\n",
      "2. Emphasize automation of repetitive tasks to reduce manual errors and increase efficiency. This could involve automating tasks such as code building, testing, and deployment.\n",
      "3. Implement continuous integration and continuous delivery (CI/CD) practices to automate the testing, building, and deployment of software products. This can help to reduce the risk of errors and improve the speed of product releases.\n",
      "4. Use monitoring and analytics to identify and resolve issues quickly. This can help to reduce downtime and improve the reliability of software products.\n",
      "\n",
      "Option 2: Implement an application performance management (APM) solution\n",
      "\n",
      "An APM solution can help to improve the performance and reliability of software products by providing real-time visibility into application health and performance. To implement an APM solution, your client should consider the following steps:\n",
      "\n",
      "1. Choose an APM solution that meets the specific needs of the company. Consider factors such as the size of the software product, the number of users, and the type of applications being developed.\n",
      "2. Implement the APM solution by following the vendor's guidelines. This may involve integrating the solution with existing systems and processes.\n",
      "3. Train development and operations teams on the use of the APM solution. This can help to ensure that the solution is used effectively to improve application performance.\n",
      "4. Use the APM solution to monitor and optimize the performance of software products. This can involve identifying performance issues, diagnosing root causes, and implementing performance improvements.\n",
      "\n",
      "By considering these options and implementing the recommended steps, your client can improve the efficiency and reliability of their software product company.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"Act as a consultant. I have a client who needing to make his software product company more efficient. \\\n",
    "    I want to impress my client by providing advice from the book Designed4Devops. \\\n",
    "    What do you recommend? \\\n",
    "    Give me two options, along with how to go about it for each\"\n",
    "}]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages,return_tensors = \"pt\",padding=True).to('cuda:0')\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens = 1000,\n",
    "    do_sample = True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617a88c-1c08-4fc0-a180-3edc72d77924",
   "metadata": {},
   "source": [
    "#### Create the LLM chain\n",
    "\n",
    "To create a symantically aware search, we need to store the context of the question, and engineer a prompt that focuses the model on answering questions using the data from our vector store instead of making it up (hallucinating). Prompt engineering is a way to coach the model into giving the sort of answers that you want return and filter those that you don't. This block sets up the chain and the template for the query that brings the context and question together to engineer the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd719b4a-e57c-4631-a523-eb91fc8536b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "designed4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52722d11-4370-4216-bbc3-946d679d38ad",
   "metadata": {},
   "source": [
    "#### Create RAG Chain\n",
    "\n",
    "This chain allows us to engineer our prompt by adding context to the question to _hopefully_ get a stronger answer. The context will come from our vector store where we embedded the book as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd6f56c-41d3-4dee-885c-4df41692b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever()\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ffbf2-ffd4-463d-9cb4-6e9e60923afa",
   "metadata": {},
   "source": [
    "## The Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f27d5e3f-9a5b-4f8b-8e54-acf50c98fca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='we process changes to our product. Digital marketplaces move quickly, so we need to introduce change', metadata={'page': 32, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='process and within the product to create short feedback loops that allow us to keep improving our product', metadata={'page': 230, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='concern yet. Improving them comes later. If you can identify separate workflows within your product, you', metadata={'page': 57, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}),\n",
       "  Document(page_content='making minor changes and integrating, testing, and releasing them more often.', metadata={'page': 125, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})],\n",
       " 'question': 'How do I improve the speed of changes that we are making to my product?',\n",
       " 'text': \"\\n### [INST] \\nInstruction: Answer the question based on your \\ndesigned4devops knowledge. Don't make up answers, just say there is no answer in the book. Here is context to help:\\n\\n[Document(page_content='we process changes to our product. Digital marketplaces move quickly, so we need to introduce change', metadata={'page': 32, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='process and within the product to create short feedback loops that allow us to keep improving our product', metadata={'page': 230, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='concern yet. Improving them comes later. If you can identify separate workflows within your product, you', metadata={'page': 57, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'}), Document(page_content='making minor changes and integrating, testing, and releasing them more often.', metadata={'page': 125, 'source': '/tf/docker-shared-data/rag-data/d4do_paperback.pdf'})]\\n\\n### QUESTION:\\nHow do I improve the speed of changes that we are making to my product? \\n\\n[/INST]\\n \\nThere are several ways to improve the speed of changes being made to a product. One approach is to break down the product into smaller, manageable workflows that can be improved independently. This allows for faster iteration and testing of individual components, which can lead to quicker overall improvements. Additionally, implementing continuous integration and delivery practices can help streamline the development process and reduce the time it takes to release new features or updates.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I improve the speed of changes that we are making to my product?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb77ca0-2e37-446d-af28-02b1f1833558",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I speed up the transfer of workflow tickets for new releases, from the development team to the operations team?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65546a1c-2df4-4d41-8e75-095a040daf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I reduce the waste in my delivery pipelines?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddec316-6230-42ad-8d28-203b94cf5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How will Designed4Devops help me cook a risotto withour burning the rice?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e567d-f404-4b4f-a23c-5bd802c17c9a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This demonstrates that the barrier to entry for prototyping generative AI servics is surprisingly low. This was achieved on relatively modest compute resources in in a few hours.\n",
    "\n",
    "Before you jump in, be sure to check out my blog on [Generative AI and RAG Security](https://www.linkedin.com/posts/methods_the-hitchhikers-guide-to-jupyter-activity-7195772472221224961-IBzo?utm_source=share&utm_medium=member_desktop).\n",
    "\n",
    "I'll be taking this project further and blogging along the way. I'll be talking about the environment I used to build this demo, how I productionise the system, package it and host it, and adding a front end so that you can interact with the book yourselves!\n",
    "\n",
    "You can download this blog as a Jupyter notebook file [here](https://github.com/tudor-james/ai-playground/blob/main/mistral-rag-langchain-chromadb.ipynb). As ever, if you need help with AI projects you can get in touch with Methods or contact us via LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd095ab-a96e-41f8-83d2-a0ba2e35ac01",
   "metadata": {},
   "source": [
    "# Adendum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b397e-5231-4814-b1c6-a9e16bd7db64",
   "metadata": {},
   "source": [
    "Let's try and make this interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c708e0a-5c47-4b33-9cbb-cc90263d8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install widgetsnbextension ipywidgets voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5f849-471d-41e9-8d06-cf7458149c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
